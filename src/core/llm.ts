import OpenAI from 'openai';
import dotenv from 'dotenv';

// åŠ è½½ç¯å¢ƒå˜é‡
dotenv.config();

// å®šä¹‰æ”¯æŒçš„LLMæä¾›å•†ç±»å‹
type SupportedProviders =
    | "openai"
    | "deepseek"
    | "qwen"
    | "modelscope"
    | "kimi"
    | "zhipu"
    | "ollama"
    | "vllm"
    | "local"
    | "auto"
    | "custom";

// è‡ªå®šä¹‰å¼‚å¸¸ç±»
export class HelloAgentsException extends Error {
    constructor(message: string) {
        super(message);
        this.name = "HelloAgentsException";
    }
}

// æ¶ˆæ¯ç±»å‹å®šä¹‰
export type ChatMessage = {
    role: "system" | "user" | "assistant" | "function";
    // function ç±»å‹çš„æ¶ˆæ¯åœ¨ OpenAI SDK ä¸­éœ€è¦ name å­—æ®µï¼Œæ•…æ­¤å¤„å…è®¸å¯é€‰ name
    name?: string;
    // content æœ‰æ—¶å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯èƒ½æ˜¯ç»“æ„åŒ–å¯¹è±¡ï¼ˆä¾‹å¦‚ function è°ƒç”¨çš„ argumentsï¼‰
    content: string | Record<string, any>;
};

// æœ¬åœ°å®šä¹‰ä¸€ä¸ªä¸ OpenAI SDK æ¶ˆæ¯ç»“æ„å…¼å®¹çš„è½»é‡ç±»å‹
type ChatCompletionMessageLike =
    | { role: 'system' | 'user' | 'assistant'; name?: undefined; content: string | null }
    | { role: 'function'; name: string; content: string | null };

/*
    ä¸ºHelloAgentså®šåˆ¶çš„LLMå®¢æˆ·ç«¯ã€‚
    å®ƒç”¨äºè°ƒç”¨ä»»ä½•å…¼å®¹OpenAIæ¥å£çš„æœåŠ¡ï¼Œå¹¶é»˜è®¤ä½¿ç”¨æµå¼å“åº”ã€‚

    è®¾è®¡ç†å¿µï¼š
    - å‚æ•°ä¼˜å…ˆï¼Œç¯å¢ƒå˜é‡å…œåº•
    - æµå¼å“åº”ä¸ºé»˜è®¤ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
    - æ”¯æŒå¤šç§LLMæä¾›å•†
    - ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
*/
export class HelloAgentsLLM {
    private model: string;
    private temperature: number;
    private maxTokens: number | undefined;
    private timeout: number;
    private kwargs: Record<string, any>;
    private provider: SupportedProviders;
    private apiKey: string;
    private baseUrl: string;
    private _client: OpenAI;

    constructor({
        model,
        apiKey,
        baseUrl,
        provider,
        temperature = 0.7,
        maxTokens,
        timeout,
        ...kwargs
    }: {
        model?: string;
        apiKey?: string;
        baseUrl?: string;
        provider?: SupportedProviders;
        temperature?: number;
        maxTokens?: number;
        timeout?: number;
        [key: string]: any;
    }) {
        // åŸºç¡€å‚æ•°åˆå§‹åŒ–
        this.temperature = temperature;
        this.maxTokens = maxTokens;
        this.timeout = timeout || parseInt(process.env.LLM_TIMEOUT || "60", 10);
        this.kwargs = kwargs;

        // è‡ªåŠ¨æ£€æµ‹æä¾›å•†
        const requestedProvider = provider?.toLowerCase() as SupportedProviders | undefined;
        this.provider = requestedProvider || this._autoDetectProvider(apiKey, baseUrl);

        // è§£æå‡­è¯ï¼ˆAPIå¯†é’¥å’ŒbaseUrlï¼‰
        if (requestedProvider === "custom") {
            this.provider = "custom";
            this.apiKey = apiKey || process.env.LLM_API_KEY || "";
            this.baseUrl = baseUrl || process.env.LLM_BASE_URL || "";
        } else {
            [this.apiKey, this.baseUrl] = this._resolveCredentials(apiKey, baseUrl);
        }

        // éªŒè¯å¿…è¦å‚æ•°å¹¶è®¾ç½®é»˜è®¤æ¨¡å‹
        this.model = model || process.env.LLM_MODEL_ID || this._getDefaultModel();
        if (!this.apiKey || !this.baseUrl) {
            throw new HelloAgentsException("APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚");
        }

        // åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        this._client = this._createClient();
    }

    /** è‡ªåŠ¨æ£€æµ‹LLMæä¾›å•† */
    private _autoDetectProvider(apiKey?: string, baseUrl?: string): SupportedProviders {
        // 1. æ£€æŸ¥ç‰¹å®šæä¾›å•†çš„ç¯å¢ƒå˜é‡
        if (process.env.OPENAI_API_KEY) return "openai";
        if (process.env.DEEPSEEK_API_KEY) return "deepseek";
        if (process.env.DASHSCOPE_API_KEY) return "qwen";
        if (process.env.MODELSCOPE_API_KEY) return "modelscope";
        if (process.env.KIMI_API_KEY || process.env.MOONSHOT_API_KEY) return "kimi";
        if (process.env.ZHIPU_API_KEY || process.env.GLM_API_KEY) return "zhipu";
        if (process.env.OLLAMA_API_KEY || process.env.OLLAMA_HOST) return "ollama";
        if (process.env.VLLM_API_KEY || process.env.VLLM_HOST) return "vllm";

        // 2. æ ¹æ®APIå¯†é’¥æ ¼å¼åˆ¤æ–­
        const actualApiKey = apiKey || process.env.LLM_API_KEY;
        if (actualApiKey) {
            if (actualApiKey.startsWith("ms-")) return "modelscope";
            if (actualApiKey.toLowerCase() === "ollama") return "ollama";
            if (actualApiKey.toLowerCase() === "vllm") return "vllm";
            if (actualApiKey.toLowerCase() === "local") return "local";
            if (actualApiKey.startsWith("sk-") && actualApiKey.length > 50) return "openai";
            if (actualApiKey.includes(".") && actualApiKey.slice(-20).includes(".")) return "zhipu";
        }

        // 3. æ ¹æ®baseUrlåˆ¤æ–­
        const actualBaseUrl = baseUrl || process.env.LLM_BASE_URL;
        if (actualBaseUrl) {
            const baseUrlLower = actualBaseUrl.toLowerCase();
            if (baseUrlLower.includes("api.openai.com")) return "openai";
            if (baseUrlLower.includes("api.deepseek.com")) return "deepseek";
            if (baseUrlLower.includes("dashscope.aliyuncs.com")) return "qwen";
            if (baseUrlLower.includes("api-inference.modelscope.cn")) return "modelscope";
            if (baseUrlLower.includes("api.moonshot.cn")) return "kimi";
            if (baseUrlLower.includes("open.bigmodel.cn")) return "zhipu";
            if (baseUrlLower.includes("localhost") || baseUrlLower.includes("127.0.0.1")) {
                if (baseUrlLower.includes(":11434") || baseUrlLower.includes("ollama")) return "ollama";
                if (baseUrlLower.includes(":8000") && baseUrlLower.includes("vllm")) return "vllm";
                if (baseUrlLower.includes(":8080") || baseUrlLower.includes(":7860")) return "local";
                if (actualApiKey?.toLowerCase() === "ollama") return "ollama";
                if (actualApiKey?.toLowerCase() === "vllm") return "vllm";
                return "local";
            }
            if (["8080", "7860", "5000"].some(port => baseUrlLower.includes(`:${port}`))) return "local";
        }

        // 4. é»˜è®¤è¿”å›auto
        return "auto";
    }

    /** æ ¹æ®providerè§£æAPIå¯†é’¥å’ŒbaseUrl */
    private _resolveCredentials(apiKey?: string, baseUrl?: string): [string, string] {
        switch (this.provider) {
            case "openai":
                return [
                    apiKey || process.env.OPENAI_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://api.openai.com/v1"
                ];
            case "deepseek":
                return [
                    apiKey || process.env.DEEPSEEK_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://api.deepseek.com"
                ];
            case "qwen":
                return [
                    apiKey || process.env.DASHSCOPE_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://dashscope.aliyuncs.com/compatible-mode/v1"
                ];
            case "modelscope":
                return [
                    apiKey || process.env.MODELSCOPE_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://api-inference.modelscope.cn/v1/"
                ];
            case "kimi":
                return [
                    apiKey || process.env.KIMI_API_KEY || process.env.MOONSHOT_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://api.moonshot.cn/v1"
                ];
            case "zhipu":
                return [
                    apiKey || process.env.ZHIPU_API_KEY || process.env.GLM_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://open.bigmodel.cn/api/paas/v4"
                ];
            case "ollama":
                return [
                    apiKey || process.env.OLLAMA_API_KEY || process.env.LLM_API_KEY || "ollama",
                    baseUrl || process.env.OLLAMA_HOST || process.env.LLM_BASE_URL || "http://localhost:11434/v1"
                ];
            case "vllm":
                return [
                    apiKey || process.env.VLLM_API_KEY || process.env.LLM_API_KEY || "vllm",
                    baseUrl || process.env.VLLM_HOST || process.env.LLM_BASE_URL || "http://localhost:8000/v1"
                ];
            case "local":
                return [
                    apiKey || process.env.LLM_API_KEY || "local",
                    baseUrl || process.env.LLM_BASE_URL || "http://localhost:8000/v1"
                ];
            case "custom":
                return [
                    apiKey || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || ""
                ];
            default:
                return [
                    apiKey || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || ""
                ];
        }
    }

    /** åˆ›å»ºOpenAIå®¢æˆ·ç«¯ */
    private _createClient(): OpenAI {
        return new OpenAI({
            apiKey: this.apiKey,
            baseURL: this.baseUrl,
            timeout: this.timeout * 1000, // OpenAIå®¢æˆ·ç«¯timeoutå•ä½ä¸ºæ¯«ç§’
        });
    }

    /** è·å–é»˜è®¤æ¨¡å‹ */
    private _getDefaultModel(): string {
        switch (this.provider) {
            case "openai": return "gpt-3.5-turbo";
            case "deepseek": return "deepseek-chat";
            case "qwen": return "qwen-plus";
            case "modelscope": return "Qwen/Qwen2.5-72B-Instruct";
            case "kimi": return "moonshot-v1-8k";
            case "zhipu": return "glm-4";
            case "ollama": return "llama3.2";
            case "vllm": return "meta-llama/Llama-2-7b-chat-hf";
            case "local": return "local-model";
            case "custom": return this.model || "gpt-3.5-turbo";
            default:
                const baseUrl = process.env.LLM_BASE_URL || "";
                const baseUrlLower = baseUrl.toLowerCase();
                if (baseUrlLower.includes("modelscope")) return "Qwen/Qwen2.5-72B-Instruct";
                if (baseUrlLower.includes("deepseek")) return "deepseek-chat";
                if (baseUrlLower.includes("dashscope")) return "qwen-plus";
                if (baseUrlLower.includes("moonshot")) return "moonshot-v1-8k";
                if (baseUrlLower.includes("bigmodel")) return "glm-4";
                if (baseUrlLower.includes("ollama") || baseUrlLower.includes(":11434")) return "llama3.2";
                if (baseUrlLower.includes(":8000") || baseUrlLower.includes("vllm")) return "meta-llama/Llama-2-7b-chat-hf";
                if (baseUrlLower.includes("localhost") || baseUrlLower.includes("127.0.0.1")) return "local-model";
                return "gpt-3.5-turbo";
        }

    }

    /**
     * æµå¼è°ƒç”¨LLMï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
     * @param messages æ¶ˆæ¯åˆ—è¡¨
     * @param temperature æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–å€¼ï¼‰
     * @returns æµå¼å“åº”ç”Ÿæˆå™¨
     */
    /** å°†å†…éƒ¨ ChatMessage[] è½¬æ¢ä¸º SDK æœŸæœ›çš„æ¶ˆæ¯æ•°ç»„ï¼ˆå¹¶ç¡®ä¿ function æ¶ˆæ¯åŒ…å« nameï¼‰ */
    private _normalizeMessages(messages: ChatMessage[]): ChatCompletionMessageLike[] {
        return messages.map((m) => {
            // å°† content ç¡®ä¿ä¸º string | nullï¼Œæ»¡è¶³å¤šæ•° SDK çš„ string | null å®šä¹‰
            let contentStr: string | null;
            if (typeof m.content === 'string') contentStr = m.content;
            else {
                try {
                    contentStr = JSON.stringify(m.content);
                } catch (e) {
                    contentStr = String(m.content as any) || null;
                }
            }

            if (m.role === 'function') {
                // function æ¶ˆæ¯å¿…é¡»åŒ…å« nameï¼›è‹¥æœªæä¾›åˆ™ç”¨å ä½åé¿å…ç±»å‹é”™è¯¯
                const name = m.name ?? 'function';
                const out: ChatCompletionMessageLike = { role: 'function', name, content: contentStr };
                return out;
            }
            const out: ChatCompletionMessageLike = { role: m.role as 'system' | 'user' | 'assistant', content: contentStr };
            return out;
        });
    }
    async *think(messages: ChatMessage[], temperature?: number): AsyncGenerator<string, void, void> {
        console.log(`ğŸ§  æ­£åœ¨è°ƒç”¨ ${this.model} æ¨¡å‹...`);
        try {
            const response = await this._client.chat.completions.create({
                model: this.model,
                messages: this._normalizeMessages(messages) as unknown as any,
                temperature: temperature ?? this.temperature,
                max_tokens: this.maxTokens ?? null,
                stream: true,
                ...this.kwargs,
            });

            console.log("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:");
            for await (const chunk of response) {
                const content = chunk?.choices?.[0]?.delta?.content ?? "";
                if (content) {
                    process.stdout.write(content); // æ— ç¼“å†²è¾“å‡º
                    yield content;
                }
            }
            console.log(); // æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ
        } catch (error) {
            const errorMsg = error instanceof Error ? error.message : String(error);
            console.log(`âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: ${errorMsg}`);
            throw new HelloAgentsException(`LLMè°ƒç”¨å¤±è´¥: ${errorMsg}`);
        }
    }

    /**
     * éæµå¼è°ƒç”¨LLMï¼Œè¿”å›å®Œæ•´å“åº”
     * @param messages æ¶ˆæ¯åˆ—è¡¨
     * @param kwargs é¢å¤–å‚æ•°ï¼ˆå¯è¦†ç›–temperatureã€maxTokensï¼‰
     * @returns å®Œæ•´å“åº”æ–‡æœ¬
     */
    async invoke(
        messages: ChatMessage[],
        kwargs: { temperature?: number; maxTokens?: number;[key: string]: any } = {}
    ): Promise<string> {
        try {
            const response = await this._client.chat.completions.create({
                model: this.model,
                messages: this._normalizeMessages(messages) as unknown as any,
                temperature: kwargs.temperature ?? this.temperature,
                max_tokens: (kwargs.maxTokens ?? this.maxTokens) ?? null,
                stream: false,
                ...this.kwargs,
                ...Object.fromEntries(
                    Object.entries(kwargs).filter(([k]) => !["temperature", "maxTokens"].includes(k))
                ),
            });
            return response?.choices?.[0]?.message?.content ?? "";
        } catch (error) {
            const errorMsg = error instanceof Error ? error.message : String(error);
            throw new HelloAgentsException(`LLMè°ƒç”¨å¤±è´¥: ${errorMsg}`);
        }
    }

    /**
     * æµå¼è°ƒç”¨åˆ«åï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
     * @param messages æ¶ˆæ¯åˆ—è¡¨
     * @param kwargs é¢å¤–å‚æ•°
     * @returns æµå¼å“åº”ç”Ÿæˆå™¨
     */
    async *streamInvoke(
        messages: ChatMessage[],
        kwargs: { temperature?: number;[key: string]: any } = {}
    ): AsyncGenerator<string, void, void> {
        yield* this.think(messages, kwargs.temperature);
    }
}