import OpenAI from 'openai';
import type { ChatCompletionMessageParam } from 'openai/resources';

import type { SupportedProviders, invokeParams } from '../types/llm.js';
import { HelloAgentsException } from '../types/exceptions.js';

/**
 *  ä¸ºHelloAgentså®šåˆ¶çš„LLMå®¢æˆ·ç«¯ã€‚
 *  å®ƒç”¨äºè°ƒç”¨ä»»ä½•å…¼å®¹OpenAIæ¥å£çš„æœåŠ¡ï¼Œå¹¶é»˜è®¤ä½¿ç”¨æµå¼å“åº”ã€‚
 *
 *  è®¾è®¡ç†å¿µï¼š
 *  - å‚æ•°ä¼˜å…ˆï¼Œç¯å¢ƒå˜é‡å…œåº•
 *  - æµå¼å“åº”ä¸ºé»˜è®¤ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
 *  - æ”¯æŒå¤šç§LLMæä¾›å•†
 *  - ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
 */
export class HelloAgentsLLM {
    private model: string | undefined;
    private temperature: number;
    private maxTokens?: number | undefined;
    private timeout: number;
    private kwargs: Record<string, any>;
    private provider: SupportedProviders | undefined;
    private apiKey: string | undefined;
    private baseUrl: string | undefined;
    private _client: OpenAI | undefined;

    constructor({
        model,
        apiKey,
        baseUrl,
        provider,
        temperature = 0.7,
        maxTokens,
        timeout,
        ...kwargs
    }: {
        model?: string;
        apiKey?: string;
        baseUrl?: string;
        provider?: SupportedProviders | undefined;
        temperature?: number;
        maxTokens?: number | undefined;
        timeout?: number;
        [key: string]: any;
    } = {}
    ) {
        // ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½
        this.model = model || process.env.LLM_MODEL_ID;
        this.temperature = temperature;
        this.maxTokens = maxTokens;
        this.timeout = timeout || parseInt(process.env.LLM_TIMEOUT || "60", 10);
        this.kwargs = kwargs;

        // è‡ªåŠ¨æ£€æµ‹provideræˆ–ä½¿ç”¨æŒ‡å®šçš„provider
        const requestedProvider = provider ? provider.toLowerCase() as SupportedProviders : null;
        this.provider = requestedProvider || this._autoDetectProvider(apiKey, baseUrl);

        if (requestedProvider === "custom") {
            this.provider = "custom";
            this.apiKey = apiKey || process.env.LLM_API_KEY;
            this.baseUrl = baseUrl || process.env.LLM_BASE_URL;
        } else {
            // æ ¹æ®providerç¡®å®šAPIå¯†é’¥å’ŒbaseUrl
            [this.apiKey, this.baseUrl] = this._resolveCredentials(apiKey, baseUrl);
        }

        // éªŒè¯å¿…è¦å‚æ•°
        if (!this.model) {
            this.model = this._getDefaultModel();
        }
        if (!this.apiKey || !this.baseUrl) {
            throw new HelloAgentsException("APIå¯†é’¥å’ŒbaseUrlå¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚");
        }

        // åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        this._client = this._createClient();
    }

    /**
     * è‡ªåŠ¨æ£€æµ‹LLMæä¾›å•†
     * 
     * æ£€æµ‹é€»è¾‘ï¼š
     * 1. ä¼˜å…ˆæ£€æŸ¥ç‰¹å®šæä¾›å•†çš„ç¯å¢ƒå˜é‡
     * 2. æ ¹æ®APIå¯†é’¥æ ¼å¼åˆ¤æ–­
     * 3. æ ¹æ®base_urlåˆ¤æ–­
     * 4. é»˜è®¤è¿”å›é€šç”¨é…ç½®
     * 
     * @param apiKey å¯é€‰çš„APIå¯†é’¥
     * @param baseUrl å¯é€‰çš„baseUrl
     * @returns æ£€æµ‹åˆ°çš„LLMæä¾›å•†
     */
    private _autoDetectProvider(apiKey?: string, baseUrl?: string): SupportedProviders {
        // 1. æ£€æŸ¥ç‰¹å®šæä¾›å•†çš„ç¯å¢ƒå˜é‡
        if (process.env.OPENAI_API_KEY) return "openai";
        if (process.env.DEEPSEEK_API_KEY) return "deepseek";
        if (process.env.DASHSCOPE_API_KEY) return "qwen";
        if (process.env.MODELSCOPE_API_KEY) return "modelscope";
        if (process.env.KIMI_API_KEY || process.env.MOONSHOT_API_KEY) return "kimi";
        if (process.env.ZHIPU_API_KEY || process.env.GLM_API_KEY) return "zhipu";
        if (process.env.OLLAMA_API_KEY || process.env.OLLAMA_HOST) return "ollama";
        if (process.env.VLLM_API_KEY || process.env.VLLM_HOST) return "vllm";

        // 2. æ ¹æ®APIå¯†é’¥æ ¼å¼åˆ¤æ–­
        const actualApiKey = apiKey || process.env.LLM_API_KEY;

        if (actualApiKey) {
            const actualApiKeyLower = actualApiKey.toLowerCase();

            if (actualApiKey.startsWith("ms-")) return "modelscope";
            if (actualApiKeyLower === "ollama") return "ollama";
            if (actualApiKeyLower === "vllm") return "vllm";
            if (actualApiKeyLower === "local") return "local";
            if (actualApiKeyLower.endsWith(".") && actualApiKeyLower.slice(-20).includes(".")) return "zhipu";
        }

        // 3. æ ¹æ®baseUrlåˆ¤æ–­
        const actualBaseUrl = baseUrl || process.env.LLM_BASE_URL;

        if (actualBaseUrl) {
            const baseUrlLower = actualBaseUrl.toLowerCase();

            if (baseUrlLower.includes("api.openai.com")) return "openai";
            else if (baseUrlLower.includes("api.deepseek.com")) return "deepseek";
            else if (baseUrlLower.includes("dashscope.aliyuncs.com")) return "qwen";
            else if (baseUrlLower.includes("api-inference.modelscope.cn")) return "modelscope";
            else if (baseUrlLower.includes("api.moonshot.cn")) return "kimi";
            else if (baseUrlLower.includes("open.bigmodel.cn")) return "zhipu";
            else if (baseUrlLower.includes("localhost") || baseUrlLower.includes("127.0.0.1")) {
                // æœ¬åœ°éƒ¨ç½²æ£€æµ‹ - ä¼˜å…ˆæ£€æŸ¥ç‰¹å®šæœåŠ¡
                if (baseUrlLower.includes(":11434") || baseUrlLower.includes("ollama")) return "ollama";
                else if (baseUrlLower.includes(":8000") && baseUrlLower.includes("vllm")) return "vllm";
                else if (baseUrlLower.includes(":8080") || baseUrlLower.includes(":7860")) return "local";
                else {
                    // æ ¹æ®APIå¯†é’¥è¿›ä¸€æ­¥åˆ¤æ–­
                    if (actualApiKey && actualApiKey.toLowerCase() === "ollama") return "ollama"
                    else if (actualApiKey && actualApiKey.toLowerCase() === "vllm") return "vllm"
                    else return "local"
                }
            }
            else if (["8080", "7860", "5000"].some(port => baseUrlLower.includes(`:${port}`))) {
                // å¸¸è§çš„æœ¬åœ°éƒ¨ç½²ç«¯å£
                return "local";
            }
        }

        // 4. é»˜è®¤è¿”å›auto
        return "auto";
    }

    /** 
     * æ ¹æ®providerè§£æAPIå¯†é’¥å’ŒbaseUrl
     * @param apiKey å¯é€‰çš„APIå¯†é’¥
     * @param baseUrl å¯é€‰çš„baseUrl
     * @returns åŒ…å«APIå¯†é’¥å’ŒbaseUrlçš„å…ƒç»„
     */
    private _resolveCredentials(apiKey?: string, baseUrl?: string): readonly [string, string] {
        switch (this.provider) {
            case "openai":
                return [
                    apiKey || process.env.OPENAI_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://api.openai.com/v1"
                ];
            case "deepseek":
                return [
                    apiKey || process.env.DEEPSEEK_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://api.deepseek.com"
                ];
            case "qwen":
                return [
                    apiKey || process.env.DASHSCOPE_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://dashscope.aliyuncs.com/compatible-mode/v1"
                ];
            case "modelscope":
                return [
                    apiKey || process.env.MODELSCOPE_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://api-inference.modelscope.cn/v1/"
                ];
            case "kimi":
                return [
                    apiKey || process.env.KIMI_API_KEY || process.env.MOONSHOT_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://api.moonshot.cn/v1"
                ];
            case "zhipu":
                return [
                    apiKey || process.env.ZHIPU_API_KEY || process.env.GLM_API_KEY || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || "https://open.bigmodel.cn/api/paas/v4"
                ];
            case "ollama":
                return [
                    apiKey || process.env.OLLAMA_API_KEY || process.env.LLM_API_KEY || "ollama",
                    baseUrl || process.env.OLLAMA_HOST || process.env.LLM_BASE_URL || "http://localhost:11434/v1"
                ];
            case "vllm":
                return [
                    apiKey || process.env.VLLM_API_KEY || process.env.LLM_API_KEY || "vllm",
                    baseUrl || process.env.VLLM_HOST || process.env.LLM_BASE_URL || "http://localhost:8000/v1"
                ];
            case "local":
                return [
                    apiKey || process.env.LLM_API_KEY || "local",
                    baseUrl || process.env.LLM_BASE_URL || "http://localhost:8000/v1"
                ];
            case "custom":
                return [
                    apiKey || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || ""
                ];
            default:
                // autoæˆ–å…¶ä»–æƒ…å†µï¼šä½¿ç”¨é€šç”¨é…ç½®ï¼Œæ”¯æŒä»»ä½•OpenAIå…¼å®¹çš„æœåŠ¡
                return [
                    apiKey || process.env.LLM_API_KEY || "",
                    baseUrl || process.env.LLM_BASE_URL || ""
                ];
        }
    }

    /** 
     * åˆ›å»ºOpenAIå®¢æˆ·ç«¯
     * @returns OpenAIå®¢æˆ·ç«¯å®ä¾‹
     */
    private _createClient(): OpenAI {
        return new OpenAI({
            apiKey: this.apiKey,
            baseURL: this.baseUrl,
            timeout: this.timeout
        });
    }

    /** 
     * è·å–é»˜è®¤æ¨¡å‹
     * @returns é»˜è®¤æ¨¡å‹åç§°
     */
    private _getDefaultModel(): string {
        switch (this.provider) {
            case "openai": return "gpt-3.5-turbo";
            case "deepseek": return "deepseek-chat";
            case "qwen": return "qwen-plus";
            case "modelscope": return "Qwen/Qwen2.5-72B-Instruct";
            case "kimi": return "moonshot-v1-8k";
            case "zhipu": return "glm-4";
            case "ollama": return "llama3.2";  // Ollamaå¸¸ç”¨æ¨¡å‹
            case "vllm": return "meta-llama/Llama-2-7b-chat-hf";  // vLLMå¸¸ç”¨æ¨¡å‹
            case "local": return "local-model";  // æœ¬åœ°æ¨¡å‹å ä½ç¬¦
            case "custom": return this.model || "gpt-3.5-turbo";
            default:
                // autoæˆ–å…¶ä»–æƒ…å†µï¼šæ ¹æ®base_urlæ™ºèƒ½æ¨æ–­é»˜è®¤æ¨¡å‹
                const baseUrl = process.env.LLM_BASE_URL || "";
                const baseUrlLower = baseUrl.toLowerCase();

                if (baseUrlLower.includes("modelscope")) return "Qwen/Qwen2.5-72B-Instruct";
                else if (baseUrlLower.includes("deepseek")) return "deepseek-chat";
                else if (baseUrlLower.includes("dashscope")) return "qwen-plus";
                else if (baseUrlLower.includes("moonshot")) return "moonshot-v1-8k";
                else if (baseUrlLower.includes("bigmodel")) return "glm-4";
                else if (baseUrlLower.includes("ollama") || baseUrlLower.includes(":11434")) return "llama3.2";
                else if (baseUrlLower.includes("vllm") || baseUrlLower.includes(":8000")) return "meta-llama/Llama-2-7b-chat-hf";
                else if (baseUrlLower.includes("localhost") || baseUrlLower.includes("127.0.0.1")) return "local-model";
                else return "gpt-3.5-turbo";
        }
    }

    /**
     * è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›æµå¼å“åº”ã€‚
     * è¿™æ˜¯ä¸»è¦çš„è°ƒç”¨æ–¹æ³•ï¼Œé»˜è®¤ä½¿ç”¨æµå¼å“åº”ä»¥è·å¾—æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚
     *
     * @param messages æ¶ˆæ¯åˆ—è¡¨
     * @param temperature æ¸©åº¦å‚æ•°ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„å€¼
     * @yields æµå¼å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
     */
    async *think(messages: Array<ChatCompletionMessageParam>, temperature?: number): AsyncIterableIterator<string> {
        console.log(`ğŸ§  æ­£åœ¨è°ƒç”¨ ${this.model} æ¨¡å‹...`);
        try {
            const response = await this._client?.chat.completions.create({
                model: this.model || "",
                messages: messages,
                temperature: temperature ?? this.temperature,
                max_tokens: this.maxTokens ?? null,
                stream: true,
            });

            // å¤„ç†æµå¼å“åº”
            console.log("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:");
            if (!response) {
                throw new HelloAgentsException("LLM è¿”å›ç©ºå“åº”");
            }
            for await (const chunk of response) {
                const content = chunk?.choices?.[0]?.delta?.content || "";
                if (content && content.trim() !== "") {
                    process.stdout.write(content);

                    // ç¡®ä¿åœ¨ç»ˆç«¯ç¯å¢ƒä¸­ç«‹å³åˆ·æ–°
                    if (process.stdout.isTTY) {
                        // é¿å…éç»ˆç«¯ç¯å¢ƒä¸‹ï¼Œæ‰§è¡Œæ— æ„ä¹‰çš„ç©ºå­—ç¬¦ä¸²å†™å…¥æ“ä½œï¼ˆå‡å°‘å†—ä½™å¼€é”€ã€é¿å…æ½œåœ¨æ ¼å¼é—®é¢˜ï¼‰
                        process.stdout.write('\x1B[0G');  // å…‰æ ‡ç§»åˆ°è¡Œé¦–ï¼ˆä¸å½±å“å†…å®¹ï¼ŒåŒæ ·è§¦å‘åˆ·æ–°ï¼‰
                    }

                    yield content;
                }
            }
            console.log(); // æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ
        } catch (error) {
            const errorMsg = error instanceof Error ? error.message : String(error);
            console.log(`âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: ${errorMsg}`);
            throw new HelloAgentsException(`LLMè°ƒç”¨å¤±è´¥: ${errorMsg}`);
        }
    }

    /**
     * éæµå¼è°ƒç”¨LLMï¼Œè¿”å›å®Œæ•´å“åº”
     * é€‚ç”¨äºä¸éœ€è¦æµå¼è¾“å‡ºçš„åœºæ™¯
     * 
     * @param messages æ¶ˆæ¯åˆ—è¡¨
     * @param kwargs é¢å¤–å‚æ•°
     * @returns å®Œæ•´å“åº”æ–‡æœ¬
     */
    async invoke(messages: Array<ChatCompletionMessageParam>, kwargs: invokeParams = {}): Promise<string> {
        try {
            const { temperature, maxTokens, ...otherParams } = kwargs;
            const response = await this._client?.chat.completions.create({
                model: this.model || "",
                messages: messages,
                temperature: temperature ?? this.temperature,
                max_tokens: (maxTokens ?? this.maxTokens) ?? null,
                ...otherParams,
                ...this.kwargs
            });
            const content = response?.choices?.[0]?.message?.content;

            if (!content) {
                throw new HelloAgentsException("LLM è¿”å›ç©ºå“åº”");
            }

            return content;
        } catch (error) {
            const errorMsg = error instanceof Error ? error.message : String(error);
            throw new HelloAgentsException(`LLMè°ƒç”¨å¤±è´¥: ${errorMsg}`);
        }
    }

    /**
     * æµå¼è°ƒç”¨LLMçš„åˆ«åæ–¹æ³•ï¼Œä¸thinkæ–¹æ³•åŠŸèƒ½ç›¸åŒã€‚
     * ä¿æŒå‘åå…¼å®¹æ€§ã€‚
     * @param messages æ¶ˆæ¯åˆ—è¡¨
     * @param kwargs é¢å¤–å‚æ•°
     * @returns æµå¼å“åº”ç”Ÿæˆå™¨
     */
    async *streamInvoke(messages: Array<ChatCompletionMessageParam>, kwargs: invokeParams = {}): AsyncIterableIterator<string> {
        yield* this.think(messages, kwargs.temperature);
    }
}